---
layout: post
title: "Healthcare needs trustworthy LLMs: Deterministic Quoting can help"
---


Hallucinations are a barrier to adopting LLMs in mission-critical applications like healthcare.

At [Invetech](https://www.invetechgroup.com/), we’re working on “Deterministic Quoting”, a new technique that ensures quotations from source material are verbatim, not hallucinated.

!(/assets/deterministic-quoting/1.png)

In this example, everything displayed with a blue background is guaranteed to be _verbatim_ from source material. No hallucinations. LLMs remain imperfect, so it may still choose to quote the _wrong part_ of the source material, but only “real” quotations are displayed on blue - they are deterministically generated.

We think Deterministic Quoting is an “enabler” for deploying LLM-based information retrieval systems in applications where there are serious consequences to incorrect information.

Many LLM systems can be designed to deterministically quote. This article provides motivation and explains a basic implementation.
- Hallucinations Matter
- Introducing Deterministic Quoting
- How Well Does It Work?
- Applications
- Technical Details: How is it Implemented?
- Beyond the Minimalist Implementation
- Conclusion: Is This Really Ready For Healthcare?

---

## Hallucinations Matter

Many organisations want to deploy LLMs with knowledge of their data and documentation. But in healthcare, the fear and reality of hallucinations prevent safe use.

!(/assets/deterministic-quoting/hallucination.png)

All current LLMs hallucinate.

* The biggest models from OpenAI, Google, Anthropic etc. hallucinate more than 30% of the time for some use-cases <!--TODO-->
* It’s safe to assume that the next generation (ChatGPT “5”, Gemini 1.5 Ultra, etc.) will continue to hallucinate, albeit at a lower rate
* Some LLMs are trained/prompted to cite sources… but these citations themselves can also be hallucinated! This can be particularly problematic: users are more likely to trust authoritative-looking citations
* At least [one system](https://gemini.google.com/) offers a function to verify claims by querying a search engine, but this still requires vigilance from the user
* “Check your own answer” iterative methods can sometimes reduce the rate of hallucinations but only partially.
* Attaching relevant source material to the query (RAG) appears better than relying on training (due to [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference)), but even still the material is sometimes transformed when quoted in the output. 

In short, any information that passes through an LLM is potentially “tainted”.

### So what?

Naturally conservative fields like healthcare do not deploy systems that are reliable “most of the time”. Even a low rate of hallucination is enough to prevent adoption at scale. It’s unreasonable to expect users to bear the burden of independent verification – it would be inevitable that users fail to perform adequate checks.

<!--TODO-->

Eventually, LLM quality may be high enough to be trustworthy as-is, but this is not currently within sight. Until then…

## Introducing: Deterministic Quoting

Deterministic Quoting techniques bridge the LLM “trust gap”.

Applications with Deterministic Quoting provide verbatim “ground-truth” information interspersed with LLM commentary. It combines the convenience and flexibility of the LLM with reliable, trustworthy data that is guaranteed to be hallucination-free. Users benefit from the framing and commentary but can easily verify the underlying assumptions any extra action.

!(/assets/deterministic-quoting/2.png)

The “hallucination-free” guarantee is achieved by ensuring that the data displayed on the blue background has never passed through an LLM (or any non-deterministic AI model). The AI chooses which section of source material to quote, but the retrieval of that text is a traditional non-AI database lookup. The only way to guarantee that an LLM has not transformed text is to never send it through the LLM in the first place.

The approach is imperfect: the surrounding text (white background) has come directly from an LLM and therefore may still be hallucinated. Or, the AI can choose an irrelevant (but still verbatim) quote to display. Still, the result is a significant improvement: users report intuitively grasping the difference between trusted quotations and prose generated by the LLM.

## How well does it work?

In practice, our Deterministic Quoting (DQ) techniques meet the stated goal of “zero false positives” for any quoted text. That is, 100% of all text displayed in the special quote box (the blue background in the examples) is indeed verbatim, not hallucinated.

In addition, several other metrics are useful:


* Are there hallucinations in the non-DQ prose? (ie. white background)
* Was the right quote/data chosen by the LLM? Is it relevant to the question?
* Is user’s query ultimately answered correctly?

Here, however, DQ is only part of the story - the underlying LLM remains the limiting factor for quality. We don’t expect DQ to improve these metrics, our goal is to avoid regressions.

Here are some results comparing a standard RAG pipeline (Llamaindex + ChatGPT 4) with a modified version with a minimalistic implementation of DQ.

<table>
  <tr>
   <td>
   </td>
   <td>
        <strong>Baseline</strong>
   </td>
   <td>
        <strong>Goal</strong>
   </td>
   <td>
        <strong>With DQ</strong>
   </td>
  </tr>
  <tr>
   <td>
        Hallucinations in blue box:
   </td>
   <td>
        N/A
   </td>
   <td>
        0
   </td>
   <td>
        0
   </td>
  </tr>
  <tr>
   <td>
        Hallucinations outside blue box:
   </td>
   <td>
   </td>
   <td>
        Better than baseline
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>
        Was the quote/data relevant?
   </td>
   <td>
   </td>
   <td>
        Baseline
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>
        Is the user’s query answered correctly?
   </td>
   <td>
   </td>
   <td>
        Baseline
   </td>
   <td>
   </td>
  </tr>
</table>

This data is consistent with other experiments we have run: adding DQ does not appear to degrade the overall quality of answers. If anything, answer quality may slightly improve.

## Applications

DQ techniques can benefit anywhere that hallucinations are problematic – typically Information Retrieval (eg RAG) and related systems. Some hypothetical examples:
<!-- TODO more! -->
* an assistant to front-line medical staff with knowledge of best-practice diagnosis and treatment for common conditions
* an educational aid that allows students to query textbooks or other study material
* a legal assistant with knowledge of case law

In all these cases, there are serious consequences to hallucinations.

---

## Technical Details: How is it Implemented?

While implementations will vary, DQ fundamentally involves a sending LLM outputs to a separate module that replaces potentially-hallucinated quotations with verbatim copies direct from the source material. This replacement is a traditional non-AI database query, that is, it’s “deterministic”.

The simplest way to achieve this is to take a typical RAG pipeline and make some modifications.

### A “Minimalist Implementation” of DQ: a modified RAG Pipeline

Here is a typical RAG pipeline:

!(/assets/deterministic-quoting/RAG-pipeline.png)

Note how the retrieved source material passes through the LLM - and is therefore liable to be transformed (hallucinated) before it is shown to the user.

We want to fix this by adding a “deterministic lookup” of quotes after all calls to the LLM are complete. Note how the new modules are added _after_ the LLM.

!(/assets/deterministic-quoting/RAG-pipeline-with-DQ.png)

To achieve this, we make six changes to the original:

1. Chunker: modify to suit in-line quotation
2. Generate a unique reference string for each chunk
3. Retrieval: wrap chunks in a structured format - including a unique reference - before passing to the LLM.
4. Prompt: instruct the LLM to cite references for all claims and output a structured format for these references
5. Deterministic lookup: use those references to loop back to the source material to find the original quote
6. Add support to GUI

### Chunking Approach

We add two constraints to the document “chunker” - the module that splits our dataset up into indexable pieces.

First, we prefer smaller chunks that are easily displayed in and around the LLM’s commentary. In RAG with current-generation LLMs, chunks are often quite large - a page of text or more. But this simple implementation of DQ only displays whole chunks, not parts. A whole page for each quote would often be too much for users to conveniently parse, so paragraph-sized are preferred. Of course, the LLM can still quote several consecutive paragraphs where relevant.

!(/assets/deterministic-quoting/3.png)

Second, we want chunk boundaries to be logical to a user: displaying cut-off pieces of a section could be confusing. Such “semantic chunking” can be tedious[^4], depending heavily on the source material structure and format. However, it does seem to provide an improvement in the quality of some answers, presumably because there is semantic value in the structure of the document.

Like most ML systems, source data preparation and chunking are often the most time consuming to implement.

### Give Each Chunk a Unique Reference String

We create a unique reference string for each chunk. This string is used in several ways:


* stored in the datastore as metadata for each chunk
* passed into the LLM context as a “header” for its chunk inside a &lt;title> tag
* output by the LLM alongside anything it wants to quote – again in a &lt;title> tag
* (ideally, if meaningful to humans) displayed in the “blue box” as a link to the original source.

This is not always straightforward. Difficulties include:


* unstructured documents. Humans would say “half-way down page 32”
* dealing with duplicates: eg “Section 1: Introduction” may appear in multiple documents, 2 documents may have the same name, or the corpus may contain multiple revisions of the same document
* the stretch goal of making the string meaningful to humans can be difficult, depending on the document set

### Prompt Engineering

Here is an example system prompt for a system with access to our internal medical device design standards:

```
System prompt:
You are an expert Q&A system with excellent knowledge of the internal documentation of our company, Invetech.
Invetech designs medical devices, so it is critical that you always accurately quote the reference information you use to answer the query.
```

Like typical RAG systems, we instruct the LLM to only answer from the source material, not its learned “memory”. Unlike many RAG systems, however, we instruct the LLM to always cite sources, keeping “commentary” separate from the quotes.

```
Always answer the query using the provided reference information, and not prior knowledge.
The reference information is provided below as a list of "sections". Each section is encapsulated by a <quote> tag.
Each <quote> contains a <title> tag at the very beginning. Use this title when quoting.

Some rules to follow:
1. Always use the appropriate <quote>s to answer the question.
2. Include a plain English summary in answer to the query, based on the text in the relevant <quote>s. There is no need to simply repeat the quoted text, however.
```

When citing, we instruct the LLM to start with the unique reference string in a structured format. XML-style works well, but so do json or others.

```
3. Quote the whole section so the user can see where the information has come from. Always include the <quote> tags, including the title, when quoting.
4. Always start quotes with "<quote>" and end quotes with "</quote>"
5. Quote multiple sections if relevant, but always quote whole sections using the correct format.
6. Use the term "Reference information" instead of "context information"
7. Always quote whole sections verbatim, not a subset.
```


### Retrieval of Top-K Chunks

The only change to retrieval is the format used when inserting into LLM context. We use the same format we want the LLM to output – including the unique reference string from above inside &lt;title> tags. This provides the LLM with examples of the preferred format without eating up valuable context space. An example:

```
<quote>
   <title>ICD-10 Version:2019 - Chapter VI Diseases of the nervous system - G43.1</title>
   G43.1 Migraine with aura [classical migraine]
   <link>https://icd.who.int/browse10/2019/en#G43.1</link>
</quote>
```

### Deterministic Lookup

After the LLM output has been returned, quotations matching the above format are extracted. In the minimal implementation of DQ, only the unique reference string is kept. The actual quote text is discarded because it may contain hallucinations.

The application looks up the unique reference string in the chunk index. If it matches, the true quotation text is inserted into the &lt;quote> tag text. Otherwise, the unique reference string has been hallucinated – the quotation is invalid.  

When this step is complete, everything contained within a &lt;quote> tag is guaranteed to be a valid quotation directly from the source material.

### GUI Changes

To complete our minimalist implementation of DQ, we modify the GUI to clearly distinguish between quotations and LLM prose. In the examples above, we:
* put the text in a blue box
* add a user-readable reference (ideally the unique reference string)
* label it as deterministically generated
* add a link to the source

!(/assets/deterministic-quoting/4.png)

---

## Beyond the Minimalist Implementation

While the implementation above is a useful explainer there are many opportunities for improvement. 
<!---TODO: explain these briefly-->
- DQ for user inputs too
- Top-K algorithm also inserts nearby chunks - instead of overlapping chunks
- screenshots of docs
- tables
- diagrams
- multi-layered quotes
- detect no quotes in response
- allow user to ask for quotes
- more sophisticated processing of the LLM output to determine which chunks are being quoted. Instead of using the unique reference string, blocks of text can be sent to the embeddings model to determine the best match.
- iterative calls to LLM to narrow down from a large chunk to a smaller one that is suitable for displaying inline. Still needs to be deterministically generated before display.

DQ isn’t limited to RAG systems. However, implementation can be more cumbersome because some parts of the RAG pipeline (eg chunking) are required for DQ.

For example, upcoming models from Google et al. [can fit whole books in context](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) - enough to make RAG unnecessary for small corpuses. They still hallucinate, so Deterministic Quoting remains beneficial, but the source data must be chunked (ideally semantically) and indexed as it was in RAG.

## Conclusion: Is this Really Ready for Healthcare?

<!--TODO. Ideas:
- Hallucinations remain an unsolved problem, but deterministic quoting may open up new application spaces for LLMs.
- Many AI systems can be designed to “Deterministically Quote”.
- Some variation of DQ will remain useful as long as models hallucinate - even at a low rate.
- More sophisticated systems don’t just discard this text, it can (marginally) improve “are-we-looking-up-the-right-thing accuracy”-->